大家好，我是数据平台邓瞩彧，下面由我对产品做个整体介绍
* OLTP vs OLAP
** 易宝的数据平台主要是由OLTP与OLAP两大部分组成.
** 简单的说，oltp就是用于事务处理的，也就是 各业务线 交易系统 对应的 交易数据库，由我们的DBA负责；另一部分是OLAP系统，主要用于数据分析，数据决策.
** 两者是相辅相成的，那么，各业务系统的交易数据经过oltp落地后再传递到olap系统, 进而提供数据分析决策，辅助业务分析,这样就形成了一套相对的数据闭环体系。
** 今天我们要分享的各种产品，主要就是olap的
**** 包括计算能力的输出与数据能力的输出
**** 同时面向于技术,产品，运营
**** 实现不同场景下的数据分析与计算。

* 趋势
** 第一点，关于数据分析，我把它单纯的从计算层面分为3个等级
**** 第一级别就是简单的数据统计，比如最大值，最小值，平均值，分组，排序，topN，不同分为点的值，或者通过这些运算排列组合得到的，很像线性空间的定义
**** 第二级别是大家都熟知的机器学习算法，做各种分类，回归分析，异常检测等，算法比如，线性回归，岭回归，决策树， 支持向量机svm等，也就是说部分权责让渡，决策权交给数据
**** 第三级别就是深度学习, 进一步的权责让渡。在这里我有意的把机器学习与深度学习分开，当然你也可以依据别的方式归为一类
** 第二点，是关于计算时间方面的
**** 我们从原来的跨天离线分析，到现在人们越来越希望计算的响应时间可到达到小时级别，分钟级别，秒级等。
** 在易宝我就深深的能感觉到这样的2种趋势
**** 大家 对数据分析的需求 正在从 /*第一阶段*/ 简单的数据统计 过度到 /*第二阶段*/ 机器学习;
**** 时间上也越来越重视实时计算。

* 数据流动图
** 接下来，我们来看下数据平台的数据流动图，这幅图我们来简单解读下，从颜色看大致分为4种主要的颜色，其中黄色代表了我们今天要介绍的产品；蓝色，是我们的计算资源，以Spark on yarn 的方式构建的, Spark是分布式计算引擎， Yarn是Apache下粗粒度的面向CPU和内存的资源调度系统；绿色是准实时的数据流动图，黄色大多和业务与存储相关。
** 我们来从总体上了解下5产品处在什么位置.2.担任什么角色.3.在什么场景下可以解决什么问题。
   总的来说分为技术型产品和数据型产品。
*** 技术型产品包括：实时计算，批计算，迭代计算（很多机器学习的本质就是迭代计算）
*** 数据型的产品包括：多维度的数据查询与数据可视化
** 2个场景
从数据流动的角度看，主要是绿色这条线与黄色这条线
大家可以从左往右看，无论是你是实时计算还是批计算，所有数据均来源于交易系统，比如说你要对某个业务系统的性能进行分析，通过全链路调用的AOP方式，把服务自身信息与判断性能的相应信息发到kafka，然后通过Spark的时候计算，将结果写回kafka或者tsdb中，然后由dataAPI取数据，或者socket.id的方式实时的流数据，或者对接报警系统等等，这就涉及到我们的第实时计算产品，DATAAPI, 我们的实时计算7*24小时高可靠，只要你会sql就可以做实时计算。
做分析展示无论你做实无论是交易数据日志，还是调度连日志，沿着绿色和黄色的线来看数据流动。

首先我们从最上面的spark说起，基于spark我们现在在做3件事，实时计算，批计算，机器学习.我们先从机器学习说起,就是机器学习，是和信息安全联合立项，用机器学习实现基于流量的异常检测系统，这个在业界都有很成熟的方案，批处理部分已经弯沉个，这个月底就实现实时机器学习对流量进行异常检测,实时机器学习就会上线.

好，下面我们来说下基于spark实现的实时计算产品与批计算产品。


* 产品1 实时计算
关于实时计算，我们基于spark streaming,在其之上封装了一层SQL语义的表达，让大家像操作数据库一样，像操作批处理一样的去实现实时计算逻辑
比如说，你想实时的每5秒钟统计一下过去1分钟这个时间窗口内，某个API的响应性能，比如最大响应时间，最小响应时间，平均耗时等等，你不在需要编码，一条sql下去，就生产一个实时计算任务，实时的不停的把你想要的结果汇总出来,保证7*24小时高效稳定。
为大家屏蔽掉了底层的技术细节，比如消息乱序，支持日志时间和墙上时间2种方式等等
--------------------------------------------------------------------------------
* 产品2 一致性对账
下面我们来看我们的第二个产品，基于sparksql实现的，一致性对账
这个我们是做过产品调研的，大致分为2种情况，第一种，基本场景下的，AB2个系统状态不一致的，单边的A有B无，A无B有和A对应B系统多条的情况，这4种典型的场景，我们现在完全支持，对账能力在秒级百万条数据，什么概念呢，我们易宝一天的全部交易条数大概就在120万左右，5秒钟全部对完，我们现在把时间粒度进一步切细，之分小时级别，分钟级别定时核对。下一步产品规划为支持链式对账，也就是说，A和B对，没有对上的还要用其他规则继续用A和C系统对，进而缩小差值。还有一种场景是基本是实时补单，这个由交易系统自己负责，不在我们的考虑范围内，OK，关于一致性对账，稍后，请吴文祥为大家做详细的解读。
当然基于spark还有实时机器学习的产品，未来会结合流式计算，做一个实时机器学习产品发布出来。

好，我们先来说下技术上的事情，大家来看这2条线我标注为虚线，我来解释下，这样有助于大家理解为什么会有橙黄色这样的一条线，其实我们数据是可以通过数据库日志捕获的方式全部同步过去的，这样,这样我们完全可以取代主从复制，其实，上linkedin在某些数据库上也确实是就是这样做的，我们现阶段没有这样做有2个原因，1是需求的急迫性要远大于基础设施的构建，2是要保证2边数据的一致性，虽然我们已经上了一套解决方案，但是我认为还是有很多改进方案的，任何一条日志都有3个属性,日志内容本身，一个时间戳，我们还赋予了一个全局唯一的自增长ID，这就保证不丢数据或者丢数据我们可以定位到哪里丢了数据。ok，技术就说到这，下面的这些黄线就表明了，我们的很多需求数据还是直接从备库取的.

接下来我们来说说基本覆盖了全公司的，有业务量的,所有分析人员都会使用的NewReport与ETL，DataAPI， Tableau，下面我们通过一个场景来把这些工具串联起来,有些时候呢，比如说要统计公司某业务线的交易量，领导关心的不是说上一分钟是多少，大多会说昨天是多少，同比增长了多少或者减少了多少等等，大多是一个粗粒度的时间，而很多财务上的统计也是以天为单位的，这种场景下呢，这些工具都派上用场了，ETL是数据整合加工搬运的利器，支持你夸库跨表，做数据整合加工，你在ETL上设置一套数据统计逻辑，设置个定时，OK，每天都会对这些数据进行自动的汇总，处理能力，随着我们往spark上的迁移，处理能力依旧是秒级百万量，你的处理逻辑其实是一堆逻辑规则，然后依据每天的情况会产生不同的结果，那么你就需要查看这些结果，这个时候NewReport就可以上场了，支持你多维度的，行列级别权限控制，支持数据钻取，托拉拽的方式去查询你的统计结果，当你分析完这些数据想汇报给领导看的时候，tableau就可以登场了，他可以做漂亮的报表，同时支持移动端和手机端，领导想什么时候看就什么时候看，还会存在一种情况就是，刚才说的2种方式都满足不聊你，那么，OK，没问题，你希望自己加工下这些数据,那么没问题，我们有DataAPI， 为你屏蔽掉了底层的异构数据源细节，无论你从DB2取手，MySQL取数据，HBase取数据，都没问题，一种风格Restful全部搞定。

ok，这就是我们的全部产品，基本提供一条独立的数据闭环的处理能力。

接下来我们大量的提到SQL，最后我会在总结的时候总结下到底什么是SQL？在OLAP系统中SQL与OLTP中有何不同.
好，
从颜色上来说，黄色代表了我们的产品，绿色代表了实时的数据流动，橙色相当于我们的批处理模型。
** 首先我们从黄色开始说起，我们的5个产品，对应了计算能力的输出与数据能力的输出
*** 计算型产品
    实时计算，基于批计算实现的一致性对账和机器学习
实时计算产品，大家一般都知道storm，spark streaming可以做，但是里面还是很复杂的，比如一致性问题，你是用日志时间处理还是墙上时间处理，等等，这些你都需要考虑，我们在这些经验的基础上，封装了一层SQL的语义表达，也就是说，你想做实时计算，一条sql下去就会生产一个topology计算拓扑或者说是spark的一个job，在生产为你执行，经过测试，我们是可以保证7*24小时高效稳定的,全公司的链路调度系统的实时计算就用于此产品。
基于批计算的一致性对账，基于spark的处理能力，我们实现了一致性对账v1.0的版本，支持A，B两个系统状态不一致的核对，单边核对，也就是A有B无，A无B有的场景，在就是一对多的场景，以上的对账逻辑同样被我们抽象出来，你简单的配置下对账逻辑，秒级处理百万条数据,产品中心部分子系统核对在使用比如，ncpay与银行子系统等，相信不久就会覆盖整个产品中心的对账实现,未来我们还会开发2.0，进一步抽象逻辑，实现有级联关系的链式对账产品。
*** 接下来我们还看看数据型产品，主要面向技术，产品，运营等
ETL 异构数据源之间搞笑的数据加工整合聚合工具，这个工具是我们在比对了Sqoop，DataX等产品后，发现产品不是过重就是无法满足我们的需求，因为这些产品都是缺少具体的数据处理逻辑的，在我们进一步调研这些产品的时候我们发现了其中的共性问题，于是抽象出了现在的etl产品，数据处理的利器
NewReport，当ETL数据处理结束，到数据落地，我们需要多维度灵活的分，这个时候NewReport为大家提供了这样的便利，NewReport提供了多维度聚合数据的灵活查询，行列级别权限，支持数据钻取等等功能，稍后我的同事会做详细介绍。
在你用NewReport分析数据之后，恩，你可能需要给你的领导或同事，用更加直观的方式去展现你的数据，这个时候你就可以选择我们的数据可视化产品Tableau，让你更加生动的去展现你的数据，让大家一目了然的做数据分析。
当然，还会存在一种情况，你希望自己做产品，做产品，更加定制化的分析你的数据，展现你的数据，这个时候DataAPI就可以上场了，API的使用对象是其他的子系统，API提供了Restful的方式，对开发屏蔽掉了底层数据源的异构，无论你从DB2查数据，MySQL查数据，HBase查数据，Phoenix查数据，或者查看通过实时计算落地的数据，你只需要一套Restful的DataAPI就可以了
这样，以上5种工具，针对不同的使用场景，提供了不同的产品，实现一套相对完整的数据闭环分析。
** 绿色，对应了实时计算的场景，比如说，全公司使用的链路调度系统，分析不同的机器上不同业务对应的不同接口的调用情况，响应耗时，调用频率，性能分析，错误诊断等等场景，交易系统被请求，接受请求开始，调用若干子系统，比如订单系统调交易系统，交易系统调路由，路由调银行子系统，等等，然后通过AOP的统一封装发送到kafka，然后做相应的数据清洗，然后进入spark做实时计算，数据分析，然后数据落地，然后在通过DataAPI采集数据或者，同时取历史数据做比对，等等。
** 橙色，
当数据没有通过Kafka同步到HBase、HDFS的时候，我们会直接从备库中取数据，通过ETL做数据加工，统一建模，然后又NewReport，Tableau或者DataAPI做分析
* NewReport
  如ppt
* ETL
  这个etl我称之为狭义上的etl
ETL作为数据整合加工搬运的处理利器.有2点可以分享下：第一点是为什么我们要自研，第二点强调的是etl与函数式的关系
** 我们为什么要自研，因为我们比对了Sqoop， DataX等开源的ETL产品，要么比较重，要么无法满足我们的需求，比如说我们发现这些产品都缺少T，也就是数据转换这样的支持，比如说我们要做数据加密，数据列替换等场景，都无法满足我们的需求，需要我们自己写代码来完成.
** 我们调研了这些ETL产品之后，我们发现我们想要的东西像极了函数编程这种范式要解决的问题，我们都知道函数式编程强调的是函数是第一公民，但是如果你真的使用过它去解决问题，你会发现它强调的是用有限的数据结构加若干的函数组合去解决问题，那么我们需要的ETL其实就是若干个函数加一个简单的数据结构，依据这样的思想就很简单的时候了ETL这样的产品。这个一开始是我们的内部工具，现在开放出来，每天有上百个任务在运行，稍后会由我的同事边天亮做详细介绍。

* DataAPI
DataAPI

* Tableau
  当你想更加直观的以图表的方式展现数据的时候，你就应该选择tableau，同时支持pc端和移动端，未来你的老板可以随时打开手机看数据报表了。
