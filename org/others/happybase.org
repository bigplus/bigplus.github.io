#+TITLE: HappyBase
#+AUTHOR: Jerry

* Insert HBase with Python
  For this little project, we are going to use the Happybase Python package. Happybase uses HBase’s Thrift API.

For our test, we are going to create a namespace and a table in HBase. We will do this in the HBase shell. To make things simple, our table is going to have only one column family - data, and we are going to accept all defaults.

% hbase shell
hbase> create_namespace "sample_data"
hbase> create "sample_data:rfic", "data"
The data for this project was downloaded from data.indy.gov on 10 February 2016.

Batching

You can insert directly into a table with the Table#put() function. However, I recommend using Batch#put() instead. When the number of records reaches the batch_size, Batch#send() will be called. See the section on benchmarks for timing data.

When you’re done, be sure to call Batch#send() manually, to flush any remaining records to the database.

Benchmarks

I ran the program for several batch sizes and averaged the results. As you can see from the time results, we can only go so fast - a little above 2 seconds - before we cannot insert any faster.

Batch (n)	Time (s)
1	10.227
100	3.533
1,000	2.091
2,000	2.141
5,000	2.081
10,000	2.190
The code
* code
#+BEGIN_SRC Python
#!/usr/bin/env python

"""
Insert data into HBase with a Python script.
To create the table, first use the hbase shell. We are going to create a
namespace called "sample_data". The table for this script is called "rfic",
as we will be inserting Request for Information Cases from the City of
Indianapolis.
Our table will have only one column family named "data", and we are accepting
all table defaults.
Original data source
https://data.indy.gov/dataset/Request-for-Information-Cases/ts4b-8qa9
% hbase shell
hbase> create_namespace "sample_data"
hbase> create "sample_data:rfic", "data"
"""

import csv
import happybase
import time

batch_size = 1000
host = "0.0.0.0"
file_path = "Request_for_Information_Cases.csv"
namespace = "sample_data"
row_count = 0
start_time = time.time()
table_name = "rfic"


def connect_to_hbase():
    """ Connect to HBase server.
    This will use the host, namespace, table name, and batch size as defined in
    the global variables above.
    """
    conn = happybase.Connection(host = host,
        table_prefix = namespace,
        table_prefix_separator = ":")
    conn.open()
    table = conn.table(table_name)
    batch = table.batch(batch_size = batch_size)
    return conn, batch


def insert_row(batch, row):
    """ Insert a row into HBase.
    Write the row to the batch. When the batch size is reached, rows will be
    sent to the database.
    Rows have the following schema:
        [ id, keyword, subcategory, type, township, city, zip, council_district,
          opened, closed, status, origin, location ]
    """
    batch.put(row[0], { "data:kw": row[1], "data:sub": row[2], "data:type": row[3],
        "data:town": row[4], "data:city": row[5], "data:zip": row[6],
        "data:cdist": row[7], "data:open": row[8], "data:close": row[9],
        "data:status": row[10], "data:origin": row[11], "data:loc": row[12] })


def read_csv():
    csvfile = open(file_path, "r")
    csvreader = csv.reader(csvfile)
    return csvreader, csvfile


# After everything has been defined, run the script.
conn, batch = connect_to_hbase()
print "Connect to HBase. table name: %s, batch size: %i" % (table_name, batch_size)
csvreader, csvfile = read_csv()
print "Connected to file. name: %s" % (file_path)

try:
    # Loop through the rows. The first row contains column headers, so skip that
    # row. Insert all remaining rows into the database.
    for row in csvreader:
        row_count += 1
        if row_count == 1:
            pass
        else:
            insert_row(batch, row)

    # If there are any leftover rows in the batch, send them now.
    batch.send()
finally:
    # No matter what happens, close the file handle.
    csvfile.close()
    conn.close()

duration = time.time() - start_time
print "Done. row count: %i, duration: %.3f s" % (row_count, duration)
#+END_SRC
from : http://jarrettmeyer.com/2016/02/15/inserting-data-into-hbase-with-python
